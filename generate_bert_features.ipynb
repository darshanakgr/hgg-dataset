{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9af64a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import glob\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from transformers import BertTokenizer, TFBertModel\n",
    "\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import *\n",
    "\n",
    "import tensorflow_text as text\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "from transformers import create_optimizer\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "abe48cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install tensorflow-addons tensorflow-text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "573615ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6e2d8c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 12\n",
    "batch_size = 32\n",
    "max_seq_length = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "143a23e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "bert_model = TFBertModel.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5cef14d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(caption):\n",
    "    x = tokenizer(caption, truncation=True, padding='max_length', max_length=max_seq_length, return_tensors=\"tf\")\n",
    "    return x[\"input_ids\"], x[\"attention_mask\"], x[\"token_type_ids\"]\n",
    "\n",
    "def preprocess_dataset(dataset):\n",
    "    input_ids = np.zeros((len(dataset), max_seq_length))\n",
    "    attention_masks = np.zeros((len(dataset), max_seq_length))\n",
    "    token_type_ids = np.zeros((len(dataset), max_seq_length))\n",
    "    \n",
    "    dataset.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    # loop through data and tokenize everything\n",
    "    for i, row in dataset.iterrows():\n",
    "        input_ids[i, :], attention_masks[i, :], token_type_ids[i, :] = tokenize(row[\"text\"])\n",
    "        \n",
    "    return input_ids, attention_masks, token_type_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7fd3c1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(trainable_encoder=False):\n",
    "    input_ids = tf.keras.Input(shape=(max_seq_length, ), dtype='int32', name=\"input_ids\")\n",
    "    attention_mask = tf.keras.Input(shape=(max_seq_length, ), dtype='int32', name=\"attention_mask\")\n",
    "    token_type_ids = tf.keras.Input(shape=(max_seq_length, ), dtype='int32', name=\"token_type_ids\")\n",
    "\n",
    "    encoded_ouput = bert_model({\"input_ids\":input_ids, \"attention_mask\":attention_mask, \"token_type_ids\":token_type_ids})\n",
    "    output = GlobalMaxPooling1D(name=\"global_max_pooling1d\")(encoded_ouput[\"last_hidden_state\"])\n",
    "    \n",
    "    model = tf.keras.models.Model(inputs={\"input_ids\":input_ids, \"attention_mask\":attention_mask, \"token_type_ids\":token_type_ids}, outputs=output)\n",
    "\n",
    "    # freeze the encoder network\n",
    "    model.layers[2].trainable = trainable_encoder\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5c0297f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1ee7bb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "caption_files = glob.glob(\"data/captions/*.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ae423a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "for caption_file in caption_files:\n",
    "    output_file = f\"data/features/text/{caption_file.split('/')[-1].replace('.csv', '.npy')}\"\n",
    "    \n",
    "    caption_data = pd.read_csv(caption_file)\n",
    "    \n",
    "    input_ids, attention_mask, token_type_ids = preprocess_dataset(caption_data)\n",
    "    \n",
    "    features = model.predict({\n",
    "        \"input_ids\":input_ids, \"attention_mask\":attention_mask, \"token_type_ids\":token_type_ids\n",
    "    })\n",
    "    \n",
    "    np.save(output_file, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a71d7e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "aeb4bca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!chown -R 1000:1000 data/features/text/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcdb66ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
